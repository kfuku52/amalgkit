Index: .idea/libraries/R_User_Library.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+><component name=\"libraryTable\">\n  <library name=\"R User Library\">\n    <CLASSES />\n    <JAVADOC />\n    <SOURCES />\n  </library>\n</component>
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- .idea/libraries/R_User_Library.xml	(revision e3fce9a39c4680455102035e768b354948bba9be)
+++ .idea/libraries/R_User_Library.xml	(date 1619104742318)
@@ -1,7 +1,6 @@
 <component name="libraryTable">
   <library name="R User Library">
     <CLASSES />
-    <JAVADOC />
     <SOURCES />
   </library>
 </component>
\ No newline at end of file
Index: amalgkit/getfastq.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>from Bio import Entrez\nfrom urllib.error import HTTPError\nimport itertools\nimport numpy\nimport time, lxml, subprocess, os, shutil, gzip, glob, sys, re\nfrom amalgkit.util import *\n\ndef getfastq_search_term(ncbi_id, additional_search_term=None):\n    # https://www.ncbi.nlm.nih.gov/books/NBK49540/\n    if additional_search_term is None:\n        search_term = ncbi_id\n    else:\n        search_term = ncbi_id+' AND '+additional_search_term\n    #   search_term = '\"'+ncbi_id+'\"'+'[Accession]'\n    return search_term\n\ndef getfastq_getxml(search_term, save_xml=True, retmax=1000):\n    entrez_db = 'sra'\n    try:\n        sra_handle = Entrez.esearch(db=entrez_db, term=search_term, retmax=10000000)\n    except HTTPError as e:\n        print(e, '- Trying Entrez.esearch() again...')\n        sra_handle = Entrez.esearch(db=entrez_db, term=search_term, retmax=10000000)\n    sra_record = Entrez.read(sra_handle)\n    record_ids = sra_record[\"IdList\"]\n    num_record = len(record_ids)\n    print('Number of SRA records:', num_record)\n    root = None\n    for i in numpy.arange(numpy.ceil(num_record//retmax)+1):\n        start = int(i*retmax)\n        end = int(((i+1)*retmax)-1) if num_record >= int(((i+1)*retmax)-1) else num_record\n        print('processing SRA records:', start, '-', end, flush=True)\n        try:\n            handle = Entrez.efetch(db=\"sra\", id=record_ids[start:end], rettype=\"full\", retmode=\"xml\", retmax=retmax)\n        except HTTPError as e:\n            print(e, '- Trying Entrez.efetch() again...')\n            handle = Entrez.efetch(db=\"sra\", id=record_ids[start:end], rettype=\"full\", retmode=\"xml\", retmax=retmax)\n        chunk = lxml.etree.parse(handle).getroot()\n        if root is None:\n            root = chunk\n        else:\n            root.append(chunk)\n    xml_string = lxml.etree.tostring(root, pretty_print=True)\n    for line in str(xml_string).split('\\n'):\n        if '<Error>' in line:\n            print(line)\n            raise Exception(species_name, ': <Error> found in the xml.')\n    return root\n\ndef get_range(sra_stat, offset, total_sra_bp, max_bp):\n    if (total_sra_bp<=max_bp):\n        start = 1\n        end = sra_stat['total_spot']\n    else:\n        if (sra_stat['total_spot'] > (sra_stat['num_read_per_sra'] + offset)):\n            start = offset\n            end = offset + sra_stat['num_read_per_sra']\n        elif (sra_stat['total_spot'] > sra_stat['num_read_per_sra']):\n            start = sra_stat['total_spot'] - sra_stat['num_read_per_sra']\n            end = sra_stat['total_spot']\n        elif (sra_stat['total_spot'] <= sra_stat['num_read_per_sra']):\n            start = 1\n            end = sra_stat['total_spot']\n    return start,end\n\ndef concat_fastq(args, metadata, output_dir, num_bp_per_sra):\n    layout = get_layout(args, metadata)\n    inext = '.amalgkit.fastq.gz'\n    infiles = list()\n    for sra_id in metadata.df.loc[:,'run']:\n        infiles.append([ f for f in os.listdir(output_dir) if (f.endswith(inext))&(f.startswith(sra_id)) ])\n    infiles = [ item for sublist in infiles for item in sublist ]\n    num_inext_files = len(infiles)\n    if (layout=='single')&(num_inext_files==1):\n        print('Only 1', inext, 'file was detected. No concatenation will happen.', flush=True)\n        outfile = args.id+inext\n        if infiles[0]!=outfile:\n            print('Replacing ID in the output file name:', infiles[0], outfile)\n            infile_path = os.path.join(output_dir, infiles[0])\n            outfile_path = os.path.join(output_dir, outfile)\n            os.rename(infile_path, outfile_path)\n        return None\n    elif (layout=='paired')&(num_inext_files==2):\n        print('Only 1 pair of', inext, 'files were detected. No concatenation will happen.', flush=True)\n        for infile in infiles:\n            outfile = args.id+re.sub('.*(_[1-2])', '\\g<1>', infile)\n            if infile!=outfile:\n                print('Replacing ID in the output file name:', infile, outfile)\n                infile_path = os.path.join(output_dir, infile)\n                outfile_path = os.path.join(output_dir, outfile)\n                os.rename(infile_path, outfile_path)\n        return None\n    else:\n        print('Concatenating files with the extension:', inext)\n        outext = '.amalgkit.fastq.gz'\n        if layout=='single':\n            subexts = ['',]\n        elif layout=='paired':\n            subexts = ['_1','_2',]\n        for subext in subexts:\n            infiles = metadata.df['run'].replace('$', subext+inext, regex=True)\n            outfile_path = os.path.join(output_dir, args.id+subext+outext)\n            if os.path.exists(outfile_path):\n                os.remove(outfile_path)\n            #with gzip.open(outfile_path, 'wb') as outfile:\n            #    for each_infile in infiles:\n            #        with gzip.open(os.path.join(args.work_dir, each_infile), 'rb') as infile:\n            #            shutil.copyfileobj(infile, outfile) # unacceptably slow\n            if os.path.exists(outfile_path):\n                os.remove(outfile_path)\n            for infile in infiles:\n                infile_path = os.path.join(output_dir, infile)\n                assert os.path.exists(infile_path), 'Dumped fastq not found: '+infile_path\n                print('Concatenated file:', infile_path, flush=True)\n                os.system('cat \"'+infile_path+'\" >> \"'+outfile_path+'\"')\n            print('')\n        if args.remove_tmp=='yes':\n            for i in metadata.df.index:\n                sra_id = metadata.df.loc[i,'run']\n                sra_stat = get_sra_stat(sra_id, metadata, num_bp_per_sra)\n                ext = get_newest_intermediate_file_extension(sra_stat, work_dir=output_dir)\n                remove_intermediate_files(sra_stat, ext=ext, work_dir=output_dir)\n        return None\n\ndef remove_sra_files(metadata, sra_dir):\n    for sra_id in metadata.df['run']:\n        sra_pattern = os.path.join(sra_dir, sra_id+'.sra*')\n        sra_paths = glob.glob(sra_pattern)\n        if len(sra_paths)>0:\n            for sra_path in sra_paths:\n                print('Deleting:', sra_path)\n                os.remove(sra_path)\n        else:\n            print('SRA file not found. Pattern searched:', sra_pattern)\n    print('')\n\ndef get_layout(args, metadata):\n    if args.layout=='auto':\n        layouts = metadata.df['lib_layout'].unique().tolist()\n        if (len(layouts)!=1):\n            print('Detected multiple layouts in the metadata:', layouts)\n        layout = 'paired' if 'paired' in layouts else 'single'\n    else:\n        layout = args.layout\n    return layout\n\ndef remove_old_intermediate_files(sra_id, work_dir):\n    old_files = os.listdir(work_dir)\n    files = [ f for f in old_files if (f.startswith(sra_id))&(not f.endswith('.sra'))&(os.path.isfile(os.path.join(work_dir,f))) ]\n    for f in files:\n        f_path = os.path.join(work_dir, f)\n        print('Deleting old intermediate file:', f_path)\n        os.remove(f_path)\n\ndef remove_intermediate_files(sra_stat, ext, work_dir):\n    file_paths = list()\n    if sra_stat['layout']=='single':\n        file_paths.append(os.path.join(work_dir,sra_stat['sra_id']+ext))\n    elif sra_stat['layout']=='paired':\n        for i in [1,2]:\n            file_paths.append(os.path.join(work_dir,sra_stat['sra_id']+'_'+str(i)+ext))\n    for file_path in file_paths:\n        if os.path.exists(file_path):\n            print('Deleting intermediate file:', file_path)\n            os.remove(file_path)\n        else:\n            print('Tried to delete but file not found:', file_path)\n\ndef get_newest_intermediate_file_extension(sra_stat, work_dir):\n    # Order is important in this list. More downstream should come first.\n    extensions = ['.amalgkit.fastq.gz','.rename.fastq.gz','.fastp.fastq.gz','.fastq.gz']\n    if sra_stat['layout']=='single':\n        subext = ''\n    elif sra_stat['layout']=='paired':\n        subext = '_1'\n    files = os.listdir(work_dir)\n    for ext in extensions:\n        if any([ f==sra_stat['sra_id']+subext+ext for f in files ]):\n            ext_out = ext\n            break\n    assert 'ext_out' in locals(), 'None of expected extensions ('+' '.join(extensions)+') found in '+work_dir\n    return ext_out\n\ndef download_sra(sra_stat, args, work_dir, overwrite=False):\n    sra_path = os.path.join(work_dir, sra_stat['sra_id']+'.sra')\n    individual_sra_tmp_dir = os.path.join(work_dir, sra_stat['sra_id']+'/')\n\n    if os.path.exists(sra_path):\n        print('Previously-downloaded sra file was detected.')\n        if (overwrite):\n            print('Removing', sra_path)\n            print('New sra file will be downloaded.')\n            os.remove(sra_path)\n    else:\n        print('Previously-downloaded sra file was not detected. New sra file will be downloaded.')\n    if (args.ascp=='yes')&(not os.path.exists(sra_path)):\n        print('Trying to download the SRA file using ascp.')\n        sra_site = 'anonftp@ftp.ncbi.nlm.nih.gov:/sra/sra-instant/reads/ByRun/sra/'+sra_stat['sra_id'][0:3]+'/'+sra_stat['sra_id'][0:6]+'/'+sra_stat['sra_id']+'/'+sra_stat['sra_id']+'.sra'\n        ascp_command = [args.ascp_exe, '-v', '-i', '\"'+args.ascp_key+'\"', '-k', '1', '-T', '-l', '300m', sra_site, '\"'+work_dir+'\"']\n        print('Command:', ' '.join(ascp_command))\n        ascp_out = subprocess.run(ascp_command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n        #TODO: Switch to try/except for error handling\n        assert (ascp_out.returncode == 0), \"ASCP did not finish safely: {}\".format(ascp_out.stdout.decode('utf8'))\n        print('ascp stdout:')\n        print(ascp_out.stdout.decode('utf8'))\n        print('ascp stderr:')\n        print(ascp_out.stderr.decode('utf8'))\n    if not os.path.exists(sra_path):\n        print('Trying to download the SRA file using prefetch (fasp protocol).')\n        if os.path.exists(individual_sra_tmp_dir):\n            shutil.rmtree(individual_sra_tmp_dir)\n        prefetch_command = [args.prefetch_exe, '--force', 'no', '--transport', 'http', '--max-size', '100G',\n                            '--output-directory', './', str(sra_stat['sra_id'])]\n        print('Command:', ' '.join(prefetch_command))\n        prefetch_out = subprocess.run(prefetch_command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n        # In newer versions of prefetch, --output-directory is obsolete and will throw an error\n        # print(prefetch_out.returncode)\n        if (prefetch_out.returncode):\n            prefetch_command = [args.prefetch_exe, '--force', 'no', '--transport', 'http', '--max-size', '100G',\n                                sra_stat['sra_id']]\n            prefetch_out = subprocess.run(prefetch_command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n        # TODO: Switch to try/except for error handling\n        assert (prefetch_out.returncode == 0), \"prefetch did not finish safely: {}\".format(prefetch_out.stdout.decode('utf8'))\n        print('prefetch stdout:')\n        print(prefetch_out.stdout.decode('utf8'))\n        print('prefetch stderr:')\n        print(prefetch_out.stderr.decode('utf8'))\n    if (not os.path.exists(sra_path))&(not os.path.exists(os.path.join(work_dir, sra_stat['sra_id']+'/', sra_stat['sra_id']+'.sra'))):\n        print('Trying to download the SRA file using prefetch (http protocol).')\n        if os.path.exists(individual_sra_tmp_dir):\n            shutil.rmtree(individual_sra_tmp_dir)\n        prefetch_command = [args.prefetch_exe, '--force', 'no', '--transport', 'http', '--max-size', '100G', str(sra_stat['sra_id'])]\n        print('Command:', ' '.join(prefetch_command))\n        prefetch_out = subprocess.run(prefetch_command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n       #  In newer versions of prefetch, --output-directory is obsolete and will throw an error\n        if (prefetch_out.returncode):\n            prefetch_command = [args.prefetch_exe, '--force', 'no', '--transport', 'http', '--max-size', '100G',\n                                sra_stat['sra_id']]\n            prefetch_out = subprocess.run(prefetch_command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n\n        # TODO: Switch to try/except for error handling\n        assert (prefetch_out.returncode == 0), \"prefetch did not finish safely: {}\".format(prefetch_out.stdout.decode('utf8'))\n        print('prefetch stdout:')\n        print(prefetch_out.stdout.decode('utf8'))\n        print('prefetch stderr:')\n        print(prefetch_out.stderr.decode('utf8'))\n    # Move files downloaded by prefetch. This is necessary because absolute path didn't work for prefetch --output-directory\n    if os.path.exists(os.path.join('./', sra_stat['sra_id']+'/', sra_stat['sra_id']+'.sra')):\n        subprocess.run(['mv', os.path.join('./', sra_stat['sra_id']+'/', sra_stat['sra_id']+'.sra'), sra_path],\n                       stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n        shutil.rmtree(os.path.join('./', sra_stat['sra_id']+'/'))\n    elif os.path.exists(os.path.expanduser(os.path.join('~/ncbi/public/sra/', sra_stat['sra_id'] + '.sra'))):\n        subprocess.run(['mv', os.path.expanduser(os.path.join('~/ncbi/public/sra/', sra_stat['sra_id'] + '.sra')), sra_path],\n                       stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n    # Move files downloaded by ascp\n    if os.path.exists(os.path.join(individual_sra_tmp_dir, sra_stat['sra_id']+'.sra')):\n        subprocess.run(['mv', os.path.join(individual_sra_tmp_dir, sra_stat['sra_id']+'.sra'), sra_path],\n                        stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n        shutil.rmtree(individual_sra_tmp_dir)\n    assert os.path.exists(sra_path), 'SRA file download failed: '+sra_stat['sra_id']\n\ndef check_getfastq_dependency(args):\n    if args.pfd=='yes':\n        test_pfd = subprocess.run([args.pfd_exe, '-h'], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n        assert (test_pfd.returncode==0), \"parallel-fastq-dump PATH cannot be found: \"+args.pfd_exe\n        test_prefetch = subprocess.run([args.prefetch_exe, '-h'], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n        assert (test_prefetch.returncode==0), \"prefetch (SRA toolkit) PATH cannot be found: \"+args.prefetch_exe\n    if args.ascp=='yes':\n        test_ascp = subprocess.run([args.ascp_exe, '--help'], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n        assert (test_ascp.returncode==0), \"ascp (Aspera Connect) PATH cannot be found: \"+args.ascp_exe\n    if args.fastp=='yes':\n        test_fp = subprocess.run([args.fastp_exe, '--help'], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n        assert (test_fp.returncode==0), \"fastp PATH cannot be found: \"+args.fastp_exe\n    test_pigz = subprocess.run(['pigz', '--help'], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n    if test_pigz.returncode==0:\n        print('pigz found. It will be used for compression/decompression in read name formatting.')\n        gz_exe = 'pigz -p '+str(args.threads)\n        ungz_exe = 'unpigz -p'+str(args.threads)\n    else:\n        print('pigz not found. gzip/gunzip will be used for compression/decompression in read name formatting.')\n        gz_exe = 'gzip'\n        ungz_exe = 'gunzip'\n    return gz_exe,ungz_exe\n\ndef set_getfastq_directories(args, sra_id):\n    if args.work_dir.startswith('./'):\n        args.work_dir = args.work_dir.replace('.', os.getcwd())\n    if args.id is not None:\n        output_dir = os.path.join(args.work_dir, 'getfastq', args.id)\n    elif args.metadata is not None:\n        output_dir = os.path.join(args.work_dir, 'getfastq', sra_id)\n    elif args.id_list is not None:\n        output_dir = os.path.join(args.work_dir, 'getfastq', sra_id)\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n    return output_dir\n\ndef run_pfd(sra_stat, args, output_dir, seq_summary, start, end):\n    sra_path = os.path.join(output_dir, sra_stat['sra_id']+'.sra')\n    pfd_command = ['parallel-fastq-dump', '-t', str(args.threads), '--minReadLen', str(args.min_read_length), '--qual-filter-1',\n                   '--skip-technical', '--split-3', '--clip', '--gzip', '--outdir', output_dir,\n                   '--tmpdir', output_dir]\n    print('Total sampled bases:', \"{:,}\".format(sra_stat['spot_length']*(end-start+1)), 'bp')\n    pfd_command = pfd_command + ['--minSpotId', str(start), '--maxSpotId', str(end)]\n    # If sra_stat['sra_id'], not sra_path, is provided, pfd couldn't find pre-downloaded .sra files\n    # and start downloading it to $HOME/ncbi/public/sra/\n    pfd_command = pfd_command + ['-s', sra_path]\n    #pfd_command = pfd_command + ['-s', sra_stat['sra_id']]\n    print('Command:', ' '.join(pfd_command))\n    pfd_out = subprocess.run(pfd_command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n    # TODO: Switch to try/except for error handling\n    assert (pfd_out.returncode == 0), \"pfd did not finish safely: {}\".format(\n        pfd_out.stdout.decode('utf8'))\n    if args.pfd_print=='yes':\n        print('parallel-fastq-dump stdout:')\n        print(pfd_out.stdout.decode('utf8'))\n        print('parallel-fastq-dump stderr:')\n        print(pfd_out.stderr.decode('utf8'))\n    stdout = pfd_out.stdout.decode('utf8')\n    nd = [ int(line.replace('Read ','').split(' ')[0]) for line in stdout.split('\\n') if line.startswith('Read') ]\n    nr = [ int(line.replace('Rejected ','').split(' ')[0]) for line in stdout.split('\\n') if line.startswith('Rejected') ]\n    nw = [ int(line.replace('Written ','').split(' ')[0]) for line in stdout.split('\\n') if line.startswith('Written') ]\n    seq_summary['bp_dumped'].loc[sra_stat['sra_id']] += sum(nd) * sra_stat['spot_length']\n    seq_summary['bp_rejected'].loc[sra_stat['sra_id']] += sum(nr) * sra_stat['spot_length']\n    seq_summary['bp_written'].loc[sra_stat['sra_id']] += sum(nw) * sra_stat['spot_length']\n    if (sra_stat['layout']=='paired'):\n        unpaired_file = os.path.join(output_dir, sra_stat['sra_id']+'.fastq.gz')\n        if os.path.exists(unpaired_file):\n            print('layout = {}; Deleting unpaired file: {}'.format(sra_stat['layout'], unpaired_file))\n            os.remove(unpaired_file)\n        else:\n            print('Unpaired file not found:', unpaired_file)\n    return seq_summary\n\ndef run_fastp(sra_stat, args, output_dir, seq_summary):\n    print('Running fastp.')\n    inext = get_newest_intermediate_file_extension(sra_stat, work_dir=output_dir)\n    outext = '.fastp.fastq.gz'\n    if args.threads>16:\n        print('Too many threads for fastp (--threads {}). Only 16 threads will be used.'.format(args.threads))\n        fastp_thread = 16\n    else:\n        fastp_thread = args.threads\n    fp_command = ['fastp', '--thread', str(fastp_thread), '--length_required', str(args.min_read_length)] + args.fastp_option.split(' ')\n    if sra_stat['layout']=='single':\n        infile = os.path.join(output_dir,sra_stat['sra_id'])\n        fp_command = fp_command + ['--in1',infile+inext,'--out1',infile+outext]\n    elif sra_stat['layout']=='paired':\n        infile1 = os.path.join(output_dir,sra_stat['sra_id']+'_1')\n        infile2 = os.path.join(output_dir,sra_stat['sra_id']+'_2')\n        fp_command = fp_command + ['--in1',infile1+inext,'--out1',infile1+outext,'--in2',infile2+inext,'--out2',infile2+outext]\n    fp_command = [ fc for fc in fp_command if fc!='' ]\n    print('Command:', ' '.join(fp_command))\n    try:\n        fp_out = subprocess.run(fp_command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n    except subprocess.CalledProcessError as e:\n        raise RuntimeError(\"command '{}' returned with error (code {}): {}\".format(e.cmd, e.returncode, e.output))\n    if args.fastp_print=='yes':\n        print('fastp stdout:')\n        print(fp_out.stdout.decode('utf8'))\n        print('fastp stderr:')\n        print(fp_out.stderr.decode('utf8'))\n    if args.remove_tmp=='yes':\n        remove_intermediate_files(sra_stat, ext=inext, work_dir=output_dir)\n    bps = fp_out.stderr.decode('utf8').split('\\n')\n    bp_in = list()\n    bp_out = list()\n    for i in range(len(bps)):\n        if (' before filtering:' in bps[i]):\n            bp_in.append(int(bps[i+2].replace('total bases: ', '')))\n        if (' after filtering:' in bps[i])|(' aftering filtering:' in bps[i]):\n            bp_out.append(int(bps[i+2].replace('total bases: ', '')))\n    seq_summary['bp_fastp_in'].loc[sra_stat['sra_id']] += sum(bp_in)\n    seq_summary['bp_fastp_out'].loc[sra_stat['sra_id']] += sum(bp_out)\n    return seq_summary\n\ndef rename_reads(sra_stat, args, output_dir, gz_exe, ungz_exe):\n    inext = get_newest_intermediate_file_extension(sra_stat, work_dir=output_dir)\n    outext = '.rename.fastq.gz'\n    if sra_stat['layout']=='single':\n        inbase = os.path.join(output_dir,sra_stat['sra_id'])\n        if os.path.exists(inbase+inext):\n            infile = inbase+inext\n            os.system(ungz_exe+' -c \"'+infile+'\" | sed -e \"s|[[:space:]].*|/1|\" | '+gz_exe+' -c > \"'+inbase+outext+'\"')\n    elif sra_stat['layout']=='paired':\n        inbase1 = os.path.join(output_dir,sra_stat['sra_id']+'_1')\n        inbase2 = os.path.join(output_dir,sra_stat['sra_id']+'_2')\n        if os.path.exists(inbase1+inext):\n            infile1 = inbase1+inext\n            infile2 = inbase2+inext\n            os.system(ungz_exe+' -c \"'+infile1+'\" | sed -e \"s|[[:space:]].*|/1|\" | '+gz_exe+' -c > \"'+inbase1+outext+'\"')\n            os.system(ungz_exe+' -c \"'+infile2+'\" | sed -e \"s|[[:space:]].*|/2|\" | '+gz_exe+' -c > \"'+inbase2+outext+'\"')\n    if args.remove_tmp=='yes':\n        remove_intermediate_files(sra_stat, ext=inext, work_dir=output_dir)\n\ndef rename_fastq(sra_stat, output_dir, inext, outext):\n    if sra_stat['layout']=='single':\n        inbase = os.path.join(output_dir, sra_stat['sra_id'])\n        os.rename(inbase+inext, inbase+outext)\n    elif sra_stat['layout']=='paired':\n        inbase1 = os.path.join(output_dir, sra_stat['sra_id']+'_1')\n        inbase2 = os.path.join(output_dir, sra_stat['sra_id']+'_2')\n        os.rename(inbase1+inext, inbase1+outext)\n        os.rename(inbase2+inext, inbase2+outext)\n\ndef get_sra_stat(sra_id, metadata, num_bp_per_sra):\n    sra_stat = dict()\n    sra_stat['sra_id'] = sra_id\n    is_sra = (metadata.df.loc[:,'run']==sra_id)\n    assert is_sra.sum()==1, 'There are multiple metadata rows with the same SRA ID: '+sra_id\n    sra_stat['layout'] = metadata.df.loc[is_sra,'lib_layout'].values[0]\n    sra_stat['total_spot'] = int(metadata.df.loc[is_sra,'total_spots'].values[0])\n    try:\n        sra_stat['spot_length'] = int(metadata.df.loc[is_sra,'spot_length'].values[0])\n    except ValueError as e:\n        sra_stat['spot_length'] = int(int(metadata.df.loc[is_sra,'total_bases'].values[0])/int(sra_stat['total_spot']))\n        print('spot_length cannot be obtained directly from the metadata.')\n        print('Using total_bases/total_spots ( =', str(sra_stat['spot_length']), ') instead.')\n    sra_stat['num_read_per_sra'] = int(num_bp_per_sra/sra_stat['spot_length'])\n    return sra_stat\n\ndef sequence_extraction(args, sra_stat, start, end, output_dir, seq_summary, gz_exe, ungz_exe, total_sra_bp):\n    sra_id = sra_stat['sra_id']\n    if args.pfd=='yes':\n        seq_summary = run_pfd(sra_stat, args, output_dir, seq_summary, start, end)\n        seq_summary['bp_remaining'].loc[sra_id] = seq_summary['bp_dumped'].loc[sra_id] - seq_summary['bp_written'].loc[sra_id]\n    if args.fastp=='yes':\n        seq_summary = run_fastp(sra_stat, args, output_dir, seq_summary)\n        seq_summary['bp_remaining'].loc[sra_id] = seq_summary['bp_dumped'].loc[sra_id] - seq_summary['bp_fastp_out'].loc[sra_id]\n    if args.read_name=='trinity':\n        rename_reads(sra_stat, args, output_dir, gz_exe, ungz_exe)\n    inext = get_newest_intermediate_file_extension(sra_stat, work_dir=output_dir)\n    outext = '.amalgkit.fastq.gz'\n    rename_fastq(sra_stat, output_dir, inext, outext)\n    seq_summary['bp_still_available'].loc[sra_id] = sra_stat['spot_length'] * (sra_stat['total_spot'] - end)\n    seq_summary['rate_passed'].loc[sra_id] = (total_sra_bp - seq_summary['bp_remaining'].loc[sra_id]) / total_sra_bp\n    seq_summary['spot_length'].loc[sra_id] = sra_stat['spot_length']\n    seq_summary['total_spot'].loc[sra_id] = sra_stat['total_spot']\n    seq_summary['start_1st'].loc[sra_id] = start\n    seq_summary['end_1st'].loc[sra_id] = end\n    return seq_summary\n\ndef calc_2nd_ranges(total_bp_remaining, seq_summary):\n    sra_target_bp = total_bp_remaining / len(seq_summary['bp_remaining'])\n    sra_target_reads = (sra_target_bp / seq_summary['spot_length']).astype(int)\n    start_2nds = seq_summary['end_1st'] + 1\n    end_2nds = start_2nds + (sra_target_reads / seq_summary['rate_passed']).astype(int)\n    total_spots = seq_summary['total_spot']\n    spot_lengths = seq_summary['spot_length']\n    pooled_missing_bp = total_bp_remaining\n    for dummy in range(1000):\n        current_total_bp = 0\n        for sra_id in end_2nds.index:\n            pooled_missing_read = (pooled_missing_bp/spot_lengths.loc[sra_id]).astype(int)\n            if (end_2nds.loc[sra_id] + pooled_missing_read < total_spots.loc[sra_id]):\n                pooled_missing_bp = 0\n                end_2nds.loc[sra_id] = end_2nds.loc[sra_id] + pooled_missing_bp\n            elif (end_2nds.loc[sra_id] + pooled_missing_read > total_spots.loc[sra_id]):\n                pooled_missing_bp = (end_2nds.loc[sra_id] + pooled_missing_read - total_spots.loc[sra_id]) * spot_lengths.loc[sra_id]\n                end_2nds.loc[sra_id] = total_spots.loc[sra_id]\n            current_total_bp += end_2nds.loc[sra_id] * spot_lengths.loc[sra_id]\n        all_equal_total_spots = all([ e2==ts for e2,ts in zip(end_2nds,total_spots) ])\n        is_enough_read = (current_total_bp>=total_bp_remaining)\n        if all_equal_total_spots:\n            print('Reached total spots in all SRAs.', flush=True)\n            break\n        if is_enough_read:\n            print('Enough read numbers were assigned for the 2nd round sequence extraction.', flush=True)\n            break\n    seq_summary['start_2nd'] = start_2nds\n    seq_summary['end_2nd'] = end_2nds\n    return seq_summary\n\ndef print_read_stats(args, seq_summary, max_bp, individual=True):\n    print('Target size (--max_bp):', \"{:,}\".format(max_bp), 'bp')\n    if args.pfd=='yes':\n        print('Sum of fastq_dump dumped reads:', \"{:,}\".format(seq_summary['bp_dumped'].sum()), 'bp')\n        print('Sum of fastq_dump rejected reads:', \"{:,}\".format(seq_summary['bp_rejected'].sum()), 'bp')\n        print('Sum of fastq_dump written reads:', \"{:,}\".format(seq_summary['bp_written'].sum()), 'bp')\n    if args.fastp=='yes':\n        print('Sum of fastp input reads:', \"{:,}\".format(seq_summary['bp_fastp_in'].sum()), 'bp')\n        print('Sum of fastp output reads:', \"{:,}\".format(seq_summary['bp_fastp_out'].sum()), 'bp')\n    if individual:\n        print('Individual SRA IDs:', ' '.join(seq_summary['bp_dumped'].index.values))\n        read_types = list()\n        keys = list()\n        if args.pfd=='yes':\n            read_types = read_types + ['fastq_dump dumped reads', 'fastq_dump rejected reads', 'fastq_dump written reads']\n            keys = keys + ['bp_dumped', 'bp_rejected', 'bp_written']\n        if args.fastp=='yes':\n            read_types = read_types + ['fastp input reads', 'fastp output reads']\n            keys = keys + ['bp_fastp_in', 'bp_fastp_out']\n        if len(read_types)>0:\n            for rt,key in zip(read_types, keys):\n                values = [ '{:,}'.format(s) for s in seq_summary[key].values ]\n                txt = ' '.join(values)\n                print('Individual {} (bp): {}'.format(rt, txt))\n    print('')\n\ndef getfastq_metadata(args):\n    assert (args.id is None and args.id_list is None)!=(args.metadata is None), 'Either --id, --id_list or --metadata should be specified.'\n    if args.id is not None:\n        print('--id is specified. Downloading SRA metadata from Entrez.')\n        assert (args.entrez_email!='aaa@bbb.com'), \"Provide your email address. No worry, you won't get spam emails.\"\n        Entrez.email = args.entrez_email\n        sra_id = args.id\n        output_dir = set_getfastq_directories(args, sra_id)\n        search_term = getfastq_search_term(sra_id, args.entrez_additional_search_term)\n        print('Entrez search term:', search_term)\n        xml_root = getfastq_getxml(search_term)\n        metadata = Metadata.from_xml(xml_root)\n        print('Filtering SRA entry with --layout:', args.layout)\n        layout = get_layout(args, metadata)\n        metadata.df = metadata.df.loc[(metadata.df['lib_layout']==layout),:]\n        if args.sci_name is not None:\n            print('Filtering SRA entry with --sci_name:', args.sci_name)\n            metadata.df = metadata.df.loc[(metadata.df['scientific_name']==args.sci_name),:]\n        if args.save_metadata:\n            metadata.df.to_csv(os.path.join(output_dir,'metadata_'+sra_id+'.tsv'), sep='\\t', index=False)\n    if args.id_list is not None:\n        print('--id is specified. Downloading SRA metadata from Entrez.')\n        assert (args.entrez_email!='aaa@bbb.com'), \"Provide your email address. No worry, you won't get spam emails.\"\n        Entrez.email = args.entrez_email\n        sra_id_list = [line.rstrip('\\n') for line in open(args.id_list)]\n        for sra_id in sra_id_list:\n            output_dir = set_getfastq_directories(args, sra_id)\n            search_term = getfastq_search_term(sra_id, args.entrez_additional_search_term)\n            print('Entrez search term:', search_term)\n            xml_root = getfastq_getxml(search_term)\n            metadata = Metadata.from_xml(xml_root)\n            print('Filtering SRA entry with --layout:', args.layout)\n            layout = get_layout(args, metadata)\n            metadata.df = metadata.df.loc[(metadata.df['lib_layout']==layout),:]\n            if args.sci_name is not None:\n                print('Filtering SRA entry with --sci_name:', args.sci_name)\n                metadata.df = metadata.df.loc[(metadata.df['scientific_name']==args.sci_name),:]\n            if args.save_metadata:\n                metadata.df.to_csv(os.path.join(output_dir,'metadata_',sra_id,'.tsv'), sep='\\t', index=False)\n\n    if args.metadata is not None:\n        print('--metadata is specified. Reading existing metadata table.')\n        assert args.concat=='no', '--concat should be set \"no\" when --metadata is specified.'\n        metadata = load_metadata(args)\n    return metadata\n\ndef is_getfastq_output_present(args, sra_stat, output_dir):\n    if args.concat=='yes':\n        prefixes = [args.id,]\n    else:\n        prefixes = [sra_stat['sra_id'],]\n    if sra_stat['layout']=='single':\n        sub_exts = ['',]\n    elif sra_stat['layout']=='paired':\n        sub_exts = ['_1', '_2']\n    exts = ['.amalgkit.fastq.gz',]\n    is_output_present = True\n    for prefix,sub_ext,ext in itertools.product(prefixes, sub_exts, exts):\n        out_path1 = os.path.join(output_dir, prefix+sub_ext+ext)\n        out_path2 = os.path.join(output_dir, prefix+sub_ext+ext+'.safely_removed')\n        print(out_path1)\n        is_output_present *= (os.path.exists(out_path1)|os.path.exists(out_path2))\n    return is_output_present\n\ndef remove_experiment_without_run(metadata):\n    num_all_run = metadata.df.shape[0]\n    is_missing_run = (metadata.df.loc[:,'run']=='')\n    num_missing_run = is_missing_run.sum()\n    if (num_missing_run>0):\n        print('There are {} out of {} Experiments without Run ID. Removing.'.format(num_missing_run, num_all_run))\n        metadata.df = metadata.df.loc[~is_missing_run,:]\n    return metadata\n\ndef getfastq_main(args):\n    #sra_dir = os.path.join(os.path.expanduser(\"~\"), 'ncbi/public/sra')\n    gz_exe,ungz_exe = check_getfastq_dependency(args)\n    metadata = getfastq_metadata(args)\n    assert metadata.df.shape[0] > 0, 'No SRA entry found. Make sure if --id is compatible with --sci_name and --layout.'\n    metadata = remove_experiment_without_run(metadata)\n    print('SRA IDs:', ' '.join(metadata.df['run'].tolist()))\n    max_bp = int(args.max_bp.replace(',',''))\n    num_sra = metadata.df.shape[0]\n    num_bp_per_sra = int(max_bp/num_sra)\n    total_sra_bp = int(metadata.df['total_bases'].astype(int).sum())\n    offset = 10000 # https://edwards.sdsu.edu/research/fastq-dump/\n    print('Number of SRAs:', num_sra)\n    print('Total target size (--max_bp):', \"{:,}\".format(max_bp), 'bp')\n    print('Total SRA size:', \"{:,}\".format(total_sra_bp), 'bp')\n    print('Target size per SRA:', \"{:,}\".format(num_bp_per_sra), 'bp')\n    for i in metadata.df.index:\n        print('Individual SRA size :', metadata.df.loc[i,'run'], ':', \"{:,}\".format(int(metadata.df.loc[i,'total_bases'])), 'bp')\n    sra_ids = metadata.df.loc[:,'run'].values\n    seq_summary = dict()\n    keys = ['bp_dumped','bp_rejected','bp_written','bp_fastp_in','bp_fastp_out','bp_remaining','bp_still_available',\n            'spot_length','total_spot','rate_passed','start_1st','end_1st','start_2nd','end_2nd',]\n    for key in keys:\n        seq_summary[key] = pandas.Series(0, index=sra_ids)\n    for i in metadata.df.index:\n        print('')\n        start_time = time.time()\n        sra_id = metadata.df.loc[i,'run']\n        sra_stat = get_sra_stat(sra_id, metadata, num_bp_per_sra)\n        output_dir = set_getfastq_directories(args, sra_id)\n        if (is_getfastq_output_present(args, sra_stat, output_dir))&(args.redo=='no'):\n            if metadata.df.shape[0]==1:\n                print('Output file(s) detected. Exiting.  Set \"--redo yes\" for reanalysis.')\n                sys.exit()\n            else:\n                print('Output file(s) detected. Skipping {}.  Set \"--redo yes\" for reanalysis.'.format(sra_id))\n                continue\n        remove_old_intermediate_files(sra_id=sra_id, work_dir=output_dir)\n        print('SRA ID:', sra_stat['sra_id'])\n        print('Library layout:', sra_stat['layout'])\n        print('Number of reads:', \"{:,}\".format(sra_stat['total_spot']))\n        print('Single/Paired read length:', sra_stat['spot_length'], 'bp')\n        print('Total bases:', \"{:,}\".format(int(metadata.df.loc[i,'total_bases'])), 'bp')\n        download_sra(sra_stat, args, output_dir, overwrite=False)\n        start,end = get_range(sra_stat, offset, total_sra_bp, max_bp)\n        seq_summary = sequence_extraction(args, sra_stat, start, end, output_dir, seq_summary,\n                                          gz_exe, ungz_exe, total_sra_bp)\n        print('Time elapsed for 1st-round sequence extraction: {}, {:,} sec'.format(sra_stat['sra_id'], int(time.time()-start_time)))\n\n    print('\\n--- getfastq 1st-round sequence generation report ---')\n    print_read_stats(args, seq_summary, max_bp)\n    total_bp_remaining = seq_summary['bp_remaining'].sum()\n    percent_remaining = total_bp_remaining/max_bp*100\n    if args.pfd=='yes':\n        total_bp_dumped = seq_summary['bp_dumped'].sum()\n        total_bp_out = seq_summary['bp_written'].sum()\n    if args.fastp=='yes':\n        total_bp_out = seq_summary['bp_fastp_out'].sum()\n    percent_dropped = total_bp_out/total_bp_dumped*100\n    txt = '{:.2f}% of reads were dropped in the 1st-round sequence generation.'\n    print(txt.format(percent_remaining, args.tol))\n    txt = 'The amount of generated reads were {:.2f}% ({:,}/{:,}) smaller than the target size (tol={}%).'\n    print(txt.format(percent_remaining, total_bp_out, max_bp, args.tol))\n    if (percent_remaining<args.tol):\n        print('Proceeding without 2nd-round sequence extraction.')\n    else:\n        print('Starting the 2nd-round sequence extraction to compensate it.')\n        seq_summary = calc_2nd_ranges(total_bp_remaining, seq_summary)\n        ext_main = '.amalgkit.fastq.gz'\n        ext_1st_tmp = '.amalgkit_1st.fastq.gz'\n        for i in metadata.df.index:\n            print('')\n            start_time = time.time()\n            sra_id = metadata.df.loc[i,'run']\n            sra_stat = get_sra_stat(sra_id, metadata, num_bp_per_sra)\n            start = seq_summary['start_2nd'].loc[sra_id]\n            end = seq_summary['end_2nd'].loc[sra_id]\n            if (start>=end):\n                txt = '{}: All spots have been extracted in the 1st trial. Cancelling the 2nd trial. start={:,}, end={:,}'\n                print(txt.format(sra_id, start, end))\n                continue\n            rename_fastq(sra_stat, output_dir, inext=ext_main, outext=ext_1st_tmp)\n            seq_summary = sequence_extraction(args, sra_stat, start, end, output_dir, output_dir, seq_summary,\n                                              gz_exe, ungz_exe, total_sra_bp)\n            if (layout=='single'):\n                subexts = ['']\n            elif (layout=='paired'):\n                subexts = ['_1','_2']\n            for subext in subexts:\n                added_path = os.path.join(output_dir, sra_id+subext+ext_1st_tmp)\n                adding_path = os.path.join(output_dir, sra_id+subext+ext_main)\n                assert os.path.exists(added_path), 'Dumped fastq not found: '+added_path\n                assert os.path.exists(adding_path), 'Dumped fastq not found: '+adding_path\n                os.system('cat \"'+adding_path+'\" >> \"'+added_path+'\"')\n                os.remove(adding_path)\n                os.rename(added_path, adding_path)\n            print('Time elapsed for 2nd-round sequence extraction: {}, {:,} sec'.format(sra_stat['sra_id'], int(time.time()-start_time)))\n\n    print('')\n    if args.concat=='yes':\n        concat_fastq(args, metadata, output_dir, num_bp_per_sra)\n    if args.remove_sra=='yes':\n        remove_sra_files(metadata, sra_dir=output_dir)\n    else:\n        if args.pfd=='yes':\n            print('SRA files not removed:', output_dir)\n    print('\\n--- getfastq final report ---')\n    print_read_stats(args, seq_summary, max_bp)\n\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- amalgkit/getfastq.py	(revision e3fce9a39c4680455102035e768b354948bba9be)
+++ amalgkit/getfastq.py	(date 1619104388110)
@@ -679,3 +679,6 @@
     print('\n--- getfastq final report ---')
     print_read_stats(args, seq_summary, max_bp)
 
+    if args.save_metadata == 'yes':
+        metadata.df.to_csv(os.path.join(output_dir, 'metadata_' + sra_id + '.tsv'), sep='\t', index=False)
+
Index: amalgkit/quant.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import re, glob, subprocess, os, sys, numpy, pandas\nfrom Bio import Entrez\nfrom amalgkit.getfastq import getfastq_getxml, getfastq_search_term\nfrom amalgkit.util import *\n\ndef is_quant_output_present(sra_id, output_dir):\n    out_path = os.path.join(output_dir, sra_id+'_abundance.tsv')\n    is_output_present = os.path.exists(out_path)\n    return is_output_present\n\ndef quant_main(args):\n\n    #check kallisto dependency\n    try:\n        subprocess.run(['kallisto', '-h'], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n    except FileNotFoundError as e:\n        print(\",<ERROR> kallisto is not installed.\")\n        sys.exit(1)\n\n    assert (args.id is None)!=(args.metadata is None), 'Either --id or --metadata should be specified.'\n    if args.id is not None:\n        print('--id is specified.')\n        sra_id = args.id\n    # BATCHMODE\n\n    if args.batch is not None:\n        assert (args.metadata is not None), '--metadata should be specified.'\n        if os.path.exists(args.metadata):\n            \"Running amalgkit quant in batch mode\"\n            quant_path = os.path.dirname(os.path.realpath(__file__))\n            batch_script_path = quant_path + '/batch_curate.sh'\n            metadata = load_metadata(args)\n            species_array = numpy.unique(metadata.df.loc[:,'species'].values)\n            species_with_index = []\n\n            for species in species_array:\n                species_split = species.split()\n                index_filename=\"_\".join(species_split) + '.idx'\n                if os.path.exists(os.path.join(args.index_dir, index_filename)):\n                    print(\"found index file for species: \",species)\n                    species_with_index.append(species)\n                else:\n                    print(\"could not find index file for species: \",species,\"(expecting file: \",os.path.join(args.index_dir, index_filename),\". Skipping.\")\n\n            metadata_filtered=metadata.loc[metadata['species'].isin(species_with_index)]\n            SRR_array = numpy.unique(metadata.df.loc[:,'experiment'])\n            SRR_with_data = []\n            for SRR in SRR_array:\n                if os.path.exists(os.path.join(args.work_dir,'getfastq',SRR,'\"*.fastq*\"')):\n                    print(\"Found data for experiment ID: \", SRR)\n                    SRR_with_data.append(SRR)\n                else:\n                    print(\"could not find fastq file for experiment ID: \",SRR,\". Skipping.\")\n            metadata_filtered=metadata_filtered.loc[metadata_filtered['experiment'].isin(SRR_with_data)]\n            print(\"writing temporary metadata file\")\n            metadata_filtered_path = os.path.join(args.work_dir, 'metadata_tmp.tsv')\n            metadata_filtered.to_csv(metadata_filtered_path, sep='\\t', index=False)\n\n            if args.output_dir:\n                output_dir = args.output_dir\n            else:\n                output_dir = os.path.join(args.work_dir, '/quant')\n\n            subprocess.run('sbatch', batch_script_path,args.work_dir, metadata_filtered_path, output_dir, args.index_dir)\n\n            print(\"job array submitted\")\n            sys.exit()\n    print('SRA ID:', sra_id)\n\n    if args.index is None:\n        index = args.ref + \".idx\"\n    else:\n        index = args.index\n\n\n    # build index via kallisto index\n\n    if args.build_index == \"yes\":\n        if not args.ref:\n            raise ValueError(\"--build_index enabled, but no reference sequences given.\")\n        else:\n            if os.path.exists(index):\n                print('kallisto index was detected:', index)\n            else:\n                print('kallisto index was not detected. Creating:', index)\n                kallisto_out = subprocess.run([\"kallisto\", \"index\", \"--i\", index, args.ref])\n                # TODO: Switch to try/except for error handling\n                assert (kallisto_out.returncode == 0), \"kallisto did not finish safely: {}\".format(kallisto_out.stdout.decode('utf8'))\n\n    # prefer amalgkit processed files over others.\n\n    in_files = glob.glob(os.path.join(args.work_dir, 'getfastq', sra_id, sra_id + \"*.amalgkit.fastq.gz\"))\n    if not in_files:\n        in_files = glob.glob(os.path.join(args.work_dir, sra_id) + \"*.fastq*\")\n        if not in_files:\n            print('could not find input data. Exiting.')\n            sys.exit()\n    # make results directory, if not already there\n    if args.output_dir:\n        output_dir = args.output_dir\n    else:\n        output_dir = os.path.join(args.work_dir, '/quant', sra_id)\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n    if (is_quant_output_present(sra_id, output_dir))&(args.redo=='no'):\n        print('Output file(s) detected. Exiting. Set \"--redo yes\" for reanalysis.')\n        sys.exit()\n\n    # start quantification process.\n    # throws exception, if in_files still empty.\n    assert in_files, '{}: Fastq file not found. Check {}'.format(sra_id, output_dir)\n    print('Input fastq detected:', ', '.join(in_files))\n    # paired end read kallisto quant, if in_files == 2\n    if len(in_files) == 2:\n        print(\"paired end reads detected. Running in paired read mode.\")\n        kallisto_out = subprocess.run([\"kallisto\", \"quant\", \"-i\", index, \"-o\", output_dir, in_files[0], in_files[1]])\n        # TODO: Switch to try/except for error handling\n        assert (kallisto_out.returncode == 0), \"kallisto did not finish safely: {}\".format(\n            kallisto_out.stdout.decode('utf8'))\n\n    # throws exception, if more than 2 files in in_files. Could be expanded to handle more than 2 files.\n    elif len(in_files) > 2:\n        raise ValueError(\"more than 2 input files given. Refer to kallisto quant -h for more info\")\n\n    else:\n        print(\"single end reads detected. Proceeding in --single mode\")\n\n        # kallisto needs fragment length and fragment length standard deviation.\n        # if there is none supplied, check if ID matches SRA ID\n        if args.fragment_length is None:\n\n            print(\"No fragment length set.\")\n            SRA_ID_pattern= re.compile(\"[DES]R[RXP][0-9]{7}\")\n            # if it does match an SRA ID pattern, try to fetch fragment length from metadata.\n            # this uses getfastq_getxml and getfastq_search_term from getfastq, as well as Metadata from metadata\n            if SRA_ID_pattern.match(sra_id):\n                print(\"SRA-ID detected. Trying to fetch fragment length from metadata.\")\n                Entrez.email = \"test@test.com\"\n                search_term = getfastq_search_term(sra_id)\n                print('Entrez search term:', search_term)\n                xml_root = getfastq_getxml(search_term)\n                metadata = Metadata.from_xml(xml_root)\n                metadata.df = metadata.df.loc[(metadata.df['lib_layout'] == \"single\"), :]\n                nominal_length = metadata.df['nominal_length'][0]\n                # set nominal length to 200 if below 200 ...\n                if nominal_length:\n                    if nominal_length < 200:\n                        nominal_length = 200\n                # ...or if undefined.\n                else:\n                    nominal_length = 200\n            # if sra_id isn't in SRA ID format, fragment length of 200 is assumed\n            else:\n                print(\"No SRA-ID detected. Assuming fragment length.\")\n                nominal_length = 200\n\n            print(\"fragment length set to: \", nominal_length)\n            fragment_sd = nominal_length/10\n            print(\"fragment length standard deviation set to:\", fragment_sd)\n            kallisto_out = subprocess.run([\"kallisto\", \"quant\", \"--index\", index, \"-o\", output_dir, \"--single\", \"-l\", str(nominal_length), \"-s\", str(fragment_sd), in_files[0]])\n            # TODO: Switch to try/except for error handling\n            assert (kallisto_out.returncode == 0), \"kallisto did not finish safely: {}\".format(\n                kallisto_out.stdout.decode('utf8'))\n        # if fragment length is supplied by the user, kallisto quant can be run immediately\n        else:\n            print(\"fragment length set to: \", args.fragment_length)\n            fragment_sd = args.fragment_length/10\n            print(\"fragment length standard deviation set to:\", fragment_sd)\n            kallisto_out = subprocess.run([\"kallisto\", \"quant\", \"--index\", index,  \"-o\", output_dir, \"--single\", \"-l\", str(args.frament_length), \"-s\", str(fragment_sd), in_files[0]])\n            # TODO: Switch to try/except for error handling\n            assert (kallisto_out.returncode == 0), \"kallisto did not finish safely: {}\".format(\n                kallisto_out.stdout.decode('utf8'))\n    # move output to results with unique name\n    os.rename(os.path.join(output_dir, \"run_info.json\"), os.path.join(output_dir, sra_id + \"_run_info.json\"))\n    os.rename(os.path.join(output_dir, \"abundance.tsv\"), os.path.join(output_dir, sra_id + \"_abundance.tsv\"))\n    os.rename(os.path.join(output_dir, \"abundance.h5\"), os.path.join(output_dir, sra_id + \"_abundance.h5\"))\n\n    if (args.clean_fastq=='yes')&(os.path.exists(os.path.join(output_dir, sra_id + \"_abundance.tsv\"))):\n        for in_file in in_files:\n            print('Output file detected. Safely removing fastq:', in_file)\n            os.remove(in_file)\n            placeholder = open(in_file+'.safely_removed', \"w\")\n            placeholder.write(\"This fastq file was safely removed after `amalgkit quant`.\")\n            placeholder.close()\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- amalgkit/quant.py	(revision e3fce9a39c4680455102035e768b354948bba9be)
+++ amalgkit/quant.py	(date 1619105010920)
@@ -1,4 +1,4 @@
-import re, glob, subprocess, os, sys, numpy, pandas
+import re, glob, subprocess, os, sys
 from Bio import Entrez
 from amalgkit.getfastq import getfastq_getxml, getfastq_search_term
 from amalgkit.util import *
@@ -21,50 +21,11 @@
     if args.id is not None:
         print('--id is specified.')
         sra_id = args.id
-    # BATCHMODE
-
-    if args.batch is not None:
-        assert (args.metadata is not None), '--metadata should be specified.'
-        if os.path.exists(args.metadata):
-            "Running amalgkit quant in batch mode"
-            quant_path = os.path.dirname(os.path.realpath(__file__))
-            batch_script_path = quant_path + '/batch_curate.sh'
-            metadata = load_metadata(args)
-            species_array = numpy.unique(metadata.df.loc[:,'species'].values)
-            species_with_index = []
-
-            for species in species_array:
-                species_split = species.split()
-                index_filename="_".join(species_split) + '.idx'
-                if os.path.exists(os.path.join(args.index_dir, index_filename)):
-                    print("found index file for species: ",species)
-                    species_with_index.append(species)
-                else:
-                    print("could not find index file for species: ",species,"(expecting file: ",os.path.join(args.index_dir, index_filename),". Skipping.")
-
-            metadata_filtered=metadata.loc[metadata['species'].isin(species_with_index)]
-            SRR_array = numpy.unique(metadata.df.loc[:,'experiment'])
-            SRR_with_data = []
-            for SRR in SRR_array:
-                if os.path.exists(os.path.join(args.work_dir,'getfastq',SRR,'"*.fastq*"')):
-                    print("Found data for experiment ID: ", SRR)
-                    SRR_with_data.append(SRR)
-                else:
-                    print("could not find fastq file for experiment ID: ",SRR,". Skipping.")
-            metadata_filtered=metadata_filtered.loc[metadata_filtered['experiment'].isin(SRR_with_data)]
-            print("writing temporary metadata file")
-            metadata_filtered_path = os.path.join(args.work_dir, 'metadata_tmp.tsv')
-            metadata_filtered.to_csv(metadata_filtered_path, sep='\t', index=False)
-
-            if args.output_dir:
-                output_dir = args.output_dir
-            else:
-                output_dir = os.path.join(args.work_dir, '/quant')
-
-            subprocess.run('sbatch', batch_script_path,args.work_dir, metadata_filtered_path, output_dir, args.index_dir)
-
-            print("job array submitted")
-            sys.exit()
+    if args.metadata is not None:
+        print('--metadata is specified. Reading existing metadata table.')
+        assert (args.batch is not None), '--batch should be specified.'
+        metadata = load_metadata(args)
+        sra_id = metadata.df.loc[:,'run'].values[0]
     print('SRA ID:', sra_id)
 
     if args.index is None:
@@ -92,16 +53,12 @@
     in_files = glob.glob(os.path.join(args.work_dir, 'getfastq', sra_id, sra_id + "*.amalgkit.fastq.gz"))
     if not in_files:
         in_files = glob.glob(os.path.join(args.work_dir, sra_id) + "*.fastq*")
-        if not in_files:
-            print('could not find input data. Exiting.')
-            sys.exit()
+
     # make results directory, if not already there
-    if args.output_dir:
-        output_dir = args.output_dir
-    else:
-        output_dir = os.path.join(args.work_dir, '/quant', sra_id)
+    output_dir = os.path.join(args.work_dir, 'quant', sra_id)
     if not os.path.exists(output_dir):
         os.makedirs(output_dir)
+
     if (is_quant_output_present(sra_id, output_dir))&(args.redo=='no'):
         print('Output file(s) detected. Exiting. Set "--redo yes" for reanalysis.')
         sys.exit()
Index: amalgkit/amalgkit
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>#! /usr/bin/env python\n\nimport argparse\nimport sys\nimport time\n\nfrom amalgkit.__init__ import __version__\n\ndef command_metadata(args):\n    sys.stdout.write('amalgkit metadata: start\\n')\n    start = time.time()\n    from amalgkit.metadata import metadata_main\n    metadata_main(args)\n    print('Time elapsed: {:,} sec'.format(int(time.time() - start)))\n    sys.stdout.write('amalgkit metadata: end\\n')\n\n\ndef command_view(args): # TODO: Do we still need this?\n    sys.stdout.write('amalgkit view: start\\n')\n    start = time.time()\n    # from amalgkit.view import view_main\n    # view_main(args)\n    print('Hello world!')\n    print('Time elapsed: {:,} sec'.format(int(time.time() - start)))\n    sys.stdout.write('amalgkit view: end\\n')\n\n\ndef command_getfastq(args):\n    sys.stdout.write('amalgkit getfastq: start\\n')\n    start = time.time()\n    from amalgkit.getfastq import getfastq_main\n    getfastq_main(args)\n    print('Time elapsed: {:,} sec'.format(int(time.time() - start)))\n    sys.stdout.write('amalgkit getfastq: end\\n')\n\n\ndef command_quant(args):\n    sys.stdout.write('amalgkit quant: start\\n')\n    start = time.time()\n    from amalgkit.quant import quant_main\n    try:\n        quant_main(args)\n    except ValueError as err:\n        print(\"ERROR: \", err)\n        sys.exit(1)\n\n    print('Time elapsed: {:,} sec'.format(int(time.time() - start)))\n    sys.stdout.write('amalgkit quant: end\\n')\n\n\ndef command_cstmm(args):\n    sys.stdout.write('amalgkit cstmm: start\\n')\n    start = time.time()\n    from amalgkit.cstmm import cstmm_main\n    cstmm_main(args)\n    print('Time elapsed: {:,} sec'.format(int(time.time() - start)))\n    sys.stdout.write('amalgkit cstmm: end\\n')\n\ndef command_curate(args):\n    sys.stdout.write('amalgkit curate: start\\n')\n    start = time.time()\n    from amalgkit.curate import curate_main\n    curate_main(args)\n    print('Time elapsed: {:,} sec'.format(int(time.time() - start)))\n    sys.stdout.write('amalgkit curate: end\\n')\n\n\ndef command_merge(args):\n    sys.stdout.write('amalgkit merge: start\\n')\n    start = time.time()\n    from amalgkit.merge import merge_main\n    merge_main(args)\n    print('Time elapsed: {:,} sec'.format(int(time.time() - start)))\n    sys.stdout.write('amalgkit merge: end\\n')\n\n\ndef command_help(args):\n    print(parser.parse_args([args.command, '--help']))\n\n\n# Main parser\nparser = argparse.ArgumentParser(description='A toolkit for transcriptome amalgamation')\nparser.add_argument('--version', action='version', version='amalgkit version ' + __version__)\nsubparsers = parser.add_subparsers()\n\n# Sub parser: metadata\npme = subparsers.add_parser('metadata', help='see `amalgkit metadata -h`')\npme.add_argument('-c', '--config_dir', metavar='PATH', default='./', type=str, required=True, action='store',\n                 help='default=%(default)s: Path to the config directory.')\npme.add_argument('-w', '--work_dir', metavar='PATH', default='./', type=str, required=False, action='store',\n                 help='default=%(default)s: Path to the directory where intermediate and output files are generated.')\npme.add_argument('-e', '--entrez_email', metavar='aaa@bbb.com', default=None, type=str, required=True, action='store',\n                 help='default=%(default)s: Your email address.')\npme.add_argument('-p', '--publication_date', metavar='YYYY/MM/DD:YYYY/MM/DD', default='1900/01/01:TODAY', type=str,\n                 required=False, action='store',\n                 help='default=%(default)s: Range of the date that records were made public in Entrez.')\npme.add_argument('-n', '--min_nspots', metavar='INT', default=5000000, type=int, required=False, action='store',\n                 help='default=%(default)s: Minimum number of RNA-seq reads per sample.')\npme.add_argument('-a', '--tissue_detect', metavar='yes|no', default='no', type=str, required=False, action='store',\n                 help='default=%(default)s: Uses config files for tissue grouping by default. If turned on, uses natural language processing to group tissues automatically.')\npme.add_argument('-s', '--max_sample', metavar='INT', default=99999, type=int, required=False, action='store',\n                 help='default=%(default)s: Maximum number of RNA-seq data sampled for one tissue in a species.')\npme.add_argument('-o', '--overwrite', metavar='no|yes', default='no', type=str,\n                 required=False, action='store', choices=['no', 'yes'],\n                 help='default=%(default)s: Overwrite existing intermediate files.')\npme.set_defaults(handler=command_metadata)\n\n# Sub parser: view\npvi = subparsers.add_parser('view', help='see `amalgkit view -h`')\npvi.add_argument('-t', '--taxid', metavar='3400', default=None, type=int, required=True, action='store',\n                 help='default=%(default)s: NCBI Taxid (e.g., 3400 for Magnoliales).')\npvi.set_defaults(handler=command_view)\n\n# Sub parser: getfastq\npge = subparsers.add_parser('getfastq', help='see `amalgkit getfastq -h`')\npge.add_argument('-e', '--entrez_email', metavar='aaa@bbb.com', default=None, type=str, required=True, action='store',\n                 help='default=%(default)s: Your email address. Necessary for accessing NCBI database.')\npge.add_argument('--work_dir', metavar='PATH', default='./', type=str, required=False, action='store',\n                 help='default=%(default)s: Working directory.')\npge.add_argument('--id', metavar='XXXXX0000', default=None, type=str, required=False, action='store',\n                 help='default=%(default)s: BioProject/BioSample/SRR ID.')\npge.add_argument('--id_list', metavar='PATH', default=None, type=str, required=False, action='store',\n                 help='default=%(default)s: Location of file containing a list of SRA IDs. Otherwise works like --id')\npge.add_argument('--layout', metavar='single|paired|auto', default='auto', type=str, required=False, action='store',\n                 choices=['single', 'paired', 'auto'],\n                 help='default=%(default)s: Library layout of RNA-seq data to be dumped. \"auto\" prioritizes paird-end libraries if both types are available.')\npge.add_argument('--max_bp', metavar='INT', default='999,999,999,999,999', type=str, required=False, action='store',\n                 help='default=%(default)s: Target sequence size (bp) to be dumped.')\npge.add_argument('--threads', metavar='INT', default=1, type=int, required=False, action='store',\n                 help='default=%(default)s: Number of threads.')\npge.add_argument('--save_metadata', metavar='STR', default=None, type=str, required=False, action='store',\n                 help='default=%(default)s: Save metadata table.')\npge.add_argument('--min_read_length', metavar='INT', default=25, type=int, required=False, action='store',\n                 help='default=%(default)s: Minimum read length.')\npge.add_argument('--pfd', metavar='yes|no', default='yes', type=str, required=False, action='store',\n                 choices=['yes', 'no'],\n                 help='default=%(default)s: Run parallel-fastq-dump.')\npge.add_argument('--pfd_exe', metavar='PATH', default='parallel-fastq-dump', type=str, required=False, action='store',\n                 help='default=%(default)s: PATH to parallel-fastq-dump executable.')\npge.add_argument('--prefetch_exe', metavar='PATH', default='prefetch', type=str, required=False, action='store',\n                 help='default=%(default)s: PATH to prefetch executable.')\npge.add_argument('--fastp', metavar='yes|no', default='yes', type=str, required=False, action='store',\n                 choices=['yes', 'no'],\n                 help='default=%(default)s: Run fastp.')\npge.add_argument('--fastp_exe', metavar='PATH', default='fastp', type=str, required=False, action='store',\n                 help='default=%(default)s: PATH to fastp executable.')\npge.add_argument('--fastp_option', metavar='STR', default='-j /dev/null -h /dev/null', type=str, required=False,\n                 action='store',\n                 help='default=%(default)s: Options to be passed to fastp. Do not include --length_required option here. It can be specified throught --min_read_length in amalgkit. ')\npge.add_argument('--remove_sra', metavar='yes|no', default='yes', type=str, required=False, action='store',\n                 choices=['yes', 'no'],\n                 help='default=%(default)s: Remove downloaded SRA files after fastq extraction.')\npge.add_argument('--remove_tmp', metavar='yes|no', default='yes', type=str, required=False, action='store',\n                 choices=['yes', 'no'],\n                 help='default=%(default)s: Remove temporary files.')\npge.add_argument('--pfd_print', metavar='yes|no', default='yes', type=str, required=False, action='store',\n                 choices=['yes', 'no'],\n                 help='default=%(default)s: Show parallel-fastq-dump stdout and stderr.')\npge.add_argument('--fastp_print', metavar='yes|no', default='yes', type=str, required=False, action='store',\n                 choices=['yes', 'no'],\n                 help='default=%(default)s: Show fastp stdout and stderr.')\npge.add_argument('--sci_name', metavar='STR', default=None, type=str, required=False, action='store',\n                 help='default=%(default)s: Species name in case the BioProject covers multiple species. Example: \"Homo sapiens\"')\npge.add_argument('--ascp', metavar='yes|no', default='no', type=str, required=False, action='store',\n                 choices=['yes', 'no'],\n                 help='default=%(default)s: Download SRA files using ascp instead of http protocol.')\npge.add_argument('--ascp_exe', metavar='PATH', default='ascp', type=str, required=False, action='store',\n                 help='default=%(default)s: PATH to ascp executable.')\npge.add_argument('--ascp_key', metavar='PATH', default='', type=str, required=False, action='store',\n                 help='default=%(default)s: PATH to ascp key. See https://www.ncbi.nlm.nih.gov/books/NBK242625/')\npge.add_argument('--ascp_option', metavar='STR', default='-v -k 1 -T -l 300m', type=str, required=False, action='store',\n                 help='default=%(default)s: ascp options.')\npge.add_argument('--read_name', metavar='default|trinity', default='default', type=str, required=False, action='store',\n                 choices=['default', 'trinity'],\n                 help='default=%(default)s: read name formatting for downstream analysis.')\npge.add_argument('--concat', metavar='yes|no', default='no', type=str, required=False, action='store', choices=['yes', 'no'],\n                 help='default=%(default)s: concatenate SRA fastq files (e.g, for assembly).')\npge.add_argument('--entrez_additional_search_term', metavar='STR',\n                 default=None,\n                # default='\"platform illumina\"[Properties] AND \"type rnaseq\"[Filter] AND \"sra biosample\"[Filter]',\n                 type=str, required=False, action='store',\n                 help='default=%(default)s: Entrez search terms in addition to --id option to further restrict the SRA entry.')\npge.add_argument('--tol', metavar='FLOAT', default=1, type=float, required=False, action='store',\n                 help='default=%(default)s: Acceptable percentage loss of reads relative to --max_bp. If the 1st-round sequence '\n                      'generation could not produce enough reads, the 2nd-round sequence generation is activated to '\n                      'compensate the loss.')\npge.add_argument('--metadata', metavar='PATH', default=None, type=str, required=False, action='store',\n                 help='default=%(default)s: PATH to metadata table. See `amalgkit metadata -h` for details.')\npge.add_argument('--batch', metavar='INT', default=None, type=int, required=False, action='store',\n                 help='default=%(default)s: Zero-based index of metadata table (--metadata). If set, process only one SRA record. This function is intended for array job processing.')\npge.add_argument('--redo', metavar='yes|no', default='no', type=str, required=False, action='store', choices=['yes', 'no'],\n                 help='default=%(default)s: Redo the analysis even if *.amalgkit.fastq.gz is detected.')\npge.set_defaults(handler=command_getfastq)\n\n# Sub parser: quant\npqu = subparsers.add_parser('quant', help='see `amalgkit quant -h`')\n\n\n\npqu.add_argument('--ref', metavar='PATH', default=None, type=str, required=False, action='store',\n                 help='default=%(default)s: path to the reference transcriptome FASTA file. Required for Index building.')\npqu.add_argument('--id', metavar='XXXXX0000', default=None, type=str, required=False, action='store',\n                 help='default=%(default)s: SRA ID. quant automatically search .fastq or .fastq.gz file(s)')\npqu.add_argument('--index', metavar='PATH', default=None, type=str, required=False, action='store',\n                 help='default=%(default)s: path/name of kallisto index file to create.')\n\n# TODO: Is this necessary? We will be able to know this by checking --index. i.e., (args.index is None)\npqu.add_argument('--build_index', metavar='yes|no', default=\"yes\", type=str, required=False, action='store',\n                 help='default=%(default)s: builds the kallisto index from a set of reference sequences. Index needs to be provided if this is set to \"no\".')\npqu.add_argument('--index_dir', metavar='PATH', default=None, type=str, required=False, action='store',\n                 help='default=%(default)s: Working_directory/Index/.')\npqu.add_argument('-o','--output_dir', metavar='PATH', default=None, type=str, required=False, action='store',\n                 help='default=%(default)s: Working_directory/quant/SRA_ID/.')\npqu.add_argument('--fragment_length', metavar='INT', default=None, type=int, required=False, action='store',\n                 help='default=%(default)s: length of the fragment (not read length). Only required, if single end reads are used.')\npqu.add_argument('--work_dir', metavar='PATH', default='./', type=str, required=False, action='store',\n                 help='default=%(default)s: Working directory.')\npqu.add_argument('--metadata', metavar='PATH', default=None, type=str, required=False, action='store',\n                 help='default=%(default)s: PATH to metadata table. See `amalgkit metadata -h` for details.')\npqu.add_argument('--batch', metavar='INT', default=None, type=int, required=False, action='store',\n                 help='default=%(default)s: Zero-based index of metadata table (--metadata). If set, process only one SRA record. This function is intended for array job processing.')\npqu.add_argument('--clean_fastq', metavar='yes|no', default='yes', type=str, required=False, action='store', choices=['yes', 'no'],\n                 help='default=%(default)s: Remove fastq files when quant is successfully completed.')\npqu.add_argument('--redo', metavar='yes|no', default='no', type=str, required=False, action='store', choices=['yes', 'no'],\n                 help='default=%(default)s: Redo the analysis even if *_abundance.tsv is detected.')\npqu.set_defaults(handler=command_quant)\n\n# Sub parser: cstmm\n# TODO: look for a better name\npcs = subparsers.add_parser('cstmm', help='see `amalgkit cstmm -h`')\npcs.add_argument('--work_dir', metavar='PATH', default='./', type=str, required=False, action='store',\n                 help='default=%(default)s: Working directory.')\npcs.add_argument('--ortho', metavar='PATH', default='./', type=str, required=True, action='store',\n                 help='default=%(default)s: OrthoFinder results directory.')\npcs.add_argument('--count', metavar='PATH', default='./', type=str, required=True, action='store',\n                 help='default=%(default)s: Folder path to where count data is stored.')\npcs.set_defaults(handler=command_cstmm)\n\n# Sub parser: curate\npcu = subparsers.add_parser('curate', help='see `amalgkit curate -h`')\n\npcu.add_argument('--batch', metavar='INT', default=None, type=int, required=False, action='store',\n                 help='default=%(default)s: If set, need to provide --infile_dir and/or --eff_len_dir. Automatically scans input directories based on Metadata file and sends batch job do cluster (SLURM only, currently)')\npcu.add_argument('--work_dir', metavar='PATH', default='./', type=str, required=False, action='store',\n                 help='default=%(default)s: Working directory.')\npcu.add_argument('--infile', metavar='PATH', default=None, type=str, required=False, action='store',\n                 help='default=%(default)s: Name or Path of file to curate.')\npcu.add_argument('--eff_len_file', metavar='PATH', default=None, type=str, required=False, action='store',\n                 help='default=%(default)s: Name or Path of file to effective length file. Has to have same format as infile.')\npcu.add_argument('--infile_dir', metavar='PATH', default=None, type=str, required=False, action='store',\n                 help='default=%(default)s: Name or Path of file to curate.')\npcu.add_argument('--eff_len_dir', metavar='PATH', default=None, type=str, required=False, action='store',\n                 help='default=%(default)s: Name or Path of file to effective length file. Has to have same format as infile.')\npcu.add_argument('--metadata', metavar='PATH', default=None, type=str, required=False, action='store',\n                 help='default=%(default)s: Path to metadata file, obtained by amalgkit metadata command.')\npcu.add_argument('--dist_method', metavar='STR', default='pearson', type=str, required=False, action='store',\n                 help='default=%(default)s: Method for calculating distance.')\npcu.add_argument('--mapping_rate', metavar='INT', default=0.20, type=int, required=False, action='store',\n                 help='default=%(default)s: cutoff for mapping rate.')\npcu.add_argument('--cleanup', metavar='INT', default=0, type=int, required=False, action='store',\n                 help='default=%(default)s: Saves intermediate files, if 0.')\npcu.add_argument('--tissues', metavar='STR', default=None, type=str, required=False, action='store',\n                 help='default=%(default)s: List of tissues to be included.')\npcu.add_argument('--norm', metavar='fpkm|tpm|none', default='fpkm', type=str, required=False, action='store',\n                 help='default=%(default)s: Algorithm performs log-fpkm, or log-tpm transformation on raw counts. If you select \"none\", please make sure to provide log-normalized input counts of some sort.')\npcu.set_defaults(handler=command_curate)\n\n# Sub parser: merge\npmg = subparsers.add_parser('merge', help='see `amalgkit merge -h`')\n\npmg.add_argument('--work_dir', metavar='PATH', default='./', type=str, required=False, action='store',\n                 help='default=%(default)s: Working directory.')\n\npmg.set_defaults(handler=command_merge)\n\n# Sub parser: help\nparser_help = subparsers.add_parser('help', help='see `help -h`')\nparser_help.add_argument('command', help='command name which help is shown')\nparser_help.set_defaults(handler=command_help)\n\n# Handler\nargs = parser.parse_args()\nif hasattr(args, 'handler'):\n    args.handler(args)\nelse:\n    parser.print_help()\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- amalgkit/amalgkit	(revision e3fce9a39c4680455102035e768b354948bba9be)
+++ amalgkit/amalgkit	(date 1619104388186)
@@ -128,7 +128,7 @@
                  help='default=%(default)s: Target sequence size (bp) to be dumped.')
 pge.add_argument('--threads', metavar='INT', default=1, type=int, required=False, action='store',
                  help='default=%(default)s: Number of threads.')
-pge.add_argument('--save_metadata', metavar='STR', default=None, type=str, required=False, action='store',
+pge.add_argument('--save_metadata', metavar='STR', default='yes', type=str, required=False, action='store',
                  help='default=%(default)s: Save metadata table.')
 pge.add_argument('--min_read_length', metavar='INT', default=25, type=int, required=False, action='store',
                  help='default=%(default)s: Minimum read length.')
